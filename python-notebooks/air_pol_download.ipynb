{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_id = 731  # Gda≈Ñsk, ul. Wyzwolenia\n",
    "start_date = datetime(2025, 1, 1)\n",
    "end_date = datetime(2025, 3, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading list of sensors for station\n",
    "def get_sensors(station_id):\n",
    "    url = f\"https://api.gios.gov.pl/pjp-api/v1/rest/station/sensors/{station_id}\" # API URL \n",
    "    resp = requests.get(url) # GET request to the API\n",
    "    resp.raise_for_status() # Raise an error for bad responses\n",
    "    data = resp.json() # JSON response\n",
    "    sensors = data.get(\"Lista stanowisk pomiarowych dla podanej stacji\", []) # Extracting the list of sensors\n",
    "\n",
    "    if isinstance(sensors, list) and all(isinstance(s, dict) for s in sensors): # Check if the response is a list of dictionaries\n",
    "        return sensors\n",
    "    else: # If the response is not in the expected format, print an error message\n",
    "        print(\"Incorrect format\", sensors)\n",
    "        return []\n",
    "\n",
    "# Extracting archival data for a given sensor and date range\n",
    "def get_archival_data(sensor_id, date_from_str, date_to_str):\n",
    "    base_url = f\"https://api.gios.gov.pl/pjp-api/v1/rest/archivalData/getDataBySensor/{sensor_id}\"\n",
    "    params = {\n",
    "        \"dateFrom\": date_from_str,\n",
    "        \"dateTo\": date_to_str,\n",
    "        \"page\": 0, # Start from the first page\n",
    "        \"size\": 100 # Number of records per page\n",
    "    }\n",
    "\n",
    "    all_data = []\n",
    "    while True: # Loop to handle pagination\n",
    "        try: # GET request to the API with parameters\n",
    "            resp = requests.get(base_url, params=params)\n",
    "            if resp.status_code == 429: # Rate limit exceeded\n",
    "                print(\"Paused due to rate limit. Retrying in 2 seconds...\")\n",
    "                time.sleep(2) # Modify the sleep time as needed\n",
    "                continue\n",
    "\n",
    "            resp.raise_for_status()\n",
    "            data = resp.json()\n",
    "            measurements = data.get(\"Lista archiwalnych wynik√≥w pomiar√≥w\", [])\n",
    "            all_data.extend(measurements)\n",
    "\n",
    "            total_pages = data.get(\"totalPages\", 1)\n",
    "            if params[\"page\"] >= total_pages - 1:\n",
    "                break\n",
    "\n",
    "            params[\"page\"] += 1\n",
    "            time.sleep(1.2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with sensor: {sensor_id}: {e}\")\n",
    "            break\n",
    "\n",
    "    return all_data\n",
    "\n",
    "# Function to process hourly data to daily averages\n",
    "def process_hourly_to_daily(data):\n",
    "    df = pd.DataFrame(data)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    df[\"Data\"] = pd.to_datetime(df[\"Data\"])\n",
    "    df = df.rename(columns={\"Warto≈õƒá\": \"value\"})\n",
    "    df = df.dropna(subset=[\"value\"])\n",
    "    df[\"date\"] = df[\"Data\"].dt.date\n",
    "    daily_avg = df.groupby(\"date\")[\"value\"].mean().reset_index()\n",
    "    return daily_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensors = get_sensors(station_id) # Download sensors for the station\n",
    "\n",
    "result_dfs = []\n",
    "\n",
    "for sensor in sensors: # Iterate through sensors\n",
    "    sensor_id = sensor.get(\"Identyfikator stanowiska\")\n",
    "    param_formula = sensor.get(\"Wska≈∫nik - wz√≥r\")\n",
    "\n",
    "    if param_formula not in [\"PM10\", \"PM2.5\"]:\n",
    "        continue  # Skip if not PM10 or PM2.5\n",
    "\n",
    "    print(f\"üîç Sensor: {param_formula} (ID: {sensor_id})\")\n",
    "\n",
    "    current_start = start_date\n",
    "    all_records = []\n",
    "\n",
    "    while current_start <= end_date: # Iterate through date ranges\n",
    "        segment_end = min(current_start + timedelta(days=365), end_date)\n",
    "        date_from_str = current_start.strftime(\"%Y-%m-%d %H:%M\")\n",
    "        date_to_str = segment_end.strftime(\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "        print(f\"Timestamp: {date_from_str} ‚Üí {date_to_str}\")\n",
    "        data = get_archival_data(sensor_id, date_from_str, date_to_str)\n",
    "        all_records.extend(data)\n",
    "\n",
    "        current_start = segment_end + timedelta(days=1)\n",
    "        time.sleep(1.5)\n",
    "\n",
    "    daily_df = process_hourly_to_daily(all_records) # Process data to daily averages\n",
    "\n",
    "    if not daily_df.empty: # Check if data is not empty\n",
    "        daily_df.rename(columns={\"value\": param_formula}, inplace=True)\n",
    "        result_dfs.append(daily_df.set_index(\"date\"))\n",
    "\n",
    "# Data check and saving\n",
    "if result_dfs:\n",
    "    final_df = pd.concat(result_dfs, axis=1)\n",
    "    final_df.sort_index(inplace=True)\n",
    "    filename = \"pomiar2025.csv\" # Output filename\n",
    "    final_df.to_csv(filename) # Save to CSV\n",
    "    print(f\"Data saved to {filename}\")\n",
    "else:\n",
    "    print(\"No data available for the specified date range.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
