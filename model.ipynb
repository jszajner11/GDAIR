{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0326e2d6",
   "metadata": {},
   "source": [
    "## Packages import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366d0218",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f015d9",
   "metadata": {},
   "source": [
    "## Creating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd67451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "df_no_risk = pd.read_excel(\"modified_data.xlsx\", sheet_name=\"Datanorisk\")\n",
    "df_risk = pd.read_excel(\"modified_data.xlsx\", sheet_name=\"Datarisk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af0d385",
   "metadata": {},
   "source": [
    "## Creating sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cec6f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data\n",
    "def create_sequence_and_label(df, label): # Create sequences of 3 rows and assign a label\n",
    "    sequences=[] # List to store sequences\n",
    "    i = 0 # Initialize index\n",
    "    while i < len(df): # Loop through the dataframe\n",
    "        seq = df.iloc[i:i + 3].values.flatten() # Get 3 rows and flatten them into a single row\n",
    "        sequences.append(seq) \n",
    "        i = i+3 # Move to the next sequence\n",
    "    return pd.DataFrame(sequences).assign(label=label) # Assign the label\n",
    "\n",
    "# Create sequences for both classes\n",
    "df1 = create_sequence_and_label(df_no_risk, label=0)\n",
    "df2 = create_sequence_and_label(df_risk, label=1)\n",
    "\n",
    "original_cols = df_no_risk.columns.tolist() # Get the original column names\n",
    "\n",
    "# Rename columns for 3-day sequences\n",
    "column_names = (\n",
    "    original_cols +\n",
    "    [f\"{col}_2\" for col in original_cols] +\n",
    "    [f\"{col}_3\" for col in original_cols] +\n",
    "    [\"label\"]\n",
    ")\n",
    "df1.columns = column_names\n",
    "df2.columns = column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff26f19",
   "metadata": {},
   "source": [
    "## Oversampling and creating train-test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ae3221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing the dataset\n",
    "df2_oversampling = resample(df2, # Resampling label 1\n",
    "                            replace=True, \n",
    "                            n_samples=1000, # Number of samples to generate\n",
    "                            random_state=42)  # Random state for reproducibility\n",
    "\n",
    "# Concatenate both datasets\n",
    "df_balanced = pd.concat([df1, df2_oversampling])\n",
    "\n",
    "# Shuffle the dataset\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Creating X and y variables for the model\n",
    "X = df_balanced.drop('label', axis=1)\n",
    "y = df_balanced['label']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2, # 30% for testing\n",
    "                                                    random_state=42\n",
    "                                                    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27501117",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9207dfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Random Forest model with hyperparameter tuning\n",
    "param_dist = {\n",
    "    'n_estimators': randint(10, 1000), # Randomly choose between 10 and 1000 trees\n",
    "    'max_depth': [None, 10, 20, 30, 50], # Randomly choose between None and various depths\n",
    "    'min_samples_split': [2, 5, 10], # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4], # Minimum number of samples required to be at a leaf node\n",
    "    'max_features': ['sqrt', 'log2', None], # Number of features to consider when looking for the best split\n",
    "    'bootstrap': [True, False], # Whether bootstrap samples are used when building trees\n",
    "    'class_weight': [None, 'balanced'] # Weights associated with classes in the form {class_label: weight}\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV for hyperparameter tuning\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "rand_search = RandomizedSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50,  # Number of different combinations to try\n",
    "    cv=5, # 5-fold cross-validation\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    scoring='f1'  # Use F1 score as the scoring metric\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "rand_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best params:\", rand_search.best_params_)\n",
    "print(\"Best score:\", rand_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf51fc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=18,\n",
    "                                  class_weight = None,\n",
    "                                  max_depth = 30,\n",
    "                                  max_features = \"log2\",\n",
    "                                  min_samples_leaf = 1,\n",
    "                                  min_samples_split = 2,\n",
    "                                  random_state = 42) # Random state for reproducibility\n",
    "\n",
    "# Fitting the model to the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions on the test set\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38dc849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importance = rf_model.feature_importance_\n",
    "print(\"Feature Importance:\", importance)\n",
    "\n",
    "df_feat = pd.DataFrame({'feature': X.columns, 'importance': importance}).sort_values('importance', ascending=False)\n",
    "df_feat"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
